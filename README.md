## RNN Language Model

### Materials

#### RNN Related
David Ha blog: introduced some good blogs
http://blog.otoro.net/2016/09/28/hyper-networks/

RNN Turorial with RNN + Numpy and LSTM + Theano (I'm using this blog's example)
http://www.wildml.com/2015/10/recurrent-neural-networks-tutorial-part-3-backpropagation-through-time-and-vanishing-gradients/

LSTM Concept Blog from Colah. I derived the charts from here
http://colah.github.io/posts/2015-08-Understanding-LSTMs/

LSTM Tensorflow Implementation
http://r2rt.com

#### Gradient Computation Related
Tanh Gradient 
https://theclevermachine.wordpress.com/tag/tanh-function/

softmax + cross_entropy Gradient
http://math.stackexchange.com/questions/945871/derivative-of-softmax-loss-function

3 good back-prop blogs
http://cs231n.github.io/optimization-2/
http://colah.github.io/posts/2015-08-Backprop/
http://neuralnetworksanddeeplearning.com/chap2.html

